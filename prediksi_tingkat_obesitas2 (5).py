# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10feDZ6nfCR-LySlZlUd45BFDrHqmUoSb
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import streamlit as st
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

# --- Konfigurasi Awal Aplikasi Streamlit ---
st.title("Aplikasi Prediksi Tingkat Obesitas")

st.markdown("""
Aplikasi ini digunakan untuk memprediksi tingkat obesitas berdasarkan data yang dimasukkan.
Silakan isi informasi berikut:
""")

# --- PEMUATAN DATA DAN PRA-PEMROSESAN (HANYA SEKALI DI AWAL) ---
@st.cache_data # Menggunakan cache_data untuk menghindari pemrosesan ulang setiap kali interaksi Streamlit
def load_and_preprocess_data(file_path):
    data = pd.read_csv(file_path)

    # Coerce problematic columns to numeric, setting errors to NaN
    columns_to_coerce = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE'] # Tambahkan TUE juga karena bisa ada '?'
    for col in columns_to_coerce:
        data[col] = pd.to_numeric(data[col], errors='coerce')

    # Tangani nilai yang hilang dengan mengisi median untuk kolom numerik saja
    numeric_cols = data.select_dtypes(include=np.number).columns
    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())

    # Tangani duplikat
    data.drop_duplicates(inplace=True)

    # Tangani outlier menggunakan IQR untuk kolom 'Weight' (dapat diperluas ke kolom numerik lain jika diperlukan)
    Q1_weight = data['Weight'].quantile(0.25)
    Q3_weight = data['Weight'].quantile(0.75)
    IQR_weight = Q3_weight - Q1_weight
    data = data[(data['Weight'] >= (Q1_weight - 1.5 * IQR_weight)) & (data['Weight'] <= (Q3_weight + 1.5 * IQR_weight))]

    # Konversi data kategori menjadi numerik menggunakan one-hot encoding
    # Simpan nama kolom asli sebelum one-hot encoding untuk target
    original_target_col = 'NObeyesdad'
    # Jika NObeyesdad tidak ada, atau sudah di-one-hot-encoding oleh pd.read_csv (jarang)
    # Anda perlu memastikan kolom ini tidak di-one-hot-encoding dua kali

    data_processed = pd.get_dummies(data.drop(columns=[original_target_col]), drop_first=True)
    # Tambahkan kembali kolom target asli sebelum SMOTE
    data_processed[original_target_col] = data[original_target_col]


    # Tentukan fitur dan target
    # target_columns = [col for col in data_processed.columns if col.startswith('NObeyesdad_')]
    # Asumsikan 'NObeyesdad' adalah kolom target tunggal sebelum di-one-hot-encoding secara internal oleh SMOTE atau model
    # Jika NObeyesdad di-one-hot-encoding oleh get_dummies, kita harus merekonstruksi target tunggal

    # Rekonstruksi target_labels dari kolom one-hot-encoded yang akan dibuat oleh get_dummies untuk features
    # Cara paling aman adalah get_dummies pada seluruh data, lalu pilih kolom fitur dan target

    # Lakukan get_dummies pada seluruh data untuk memastikan konsistensi kolom
    full_data_encoded = pd.get_dummies(data, drop_first=True)

    target_columns_encoded = [col for col in full_data_encoded.columns if col.startswith('NObeyesdad_')]
    features = full_data_encoded.drop(columns=target_columns_encoded)

    # Target untuk SMOTE harus berupa series categorical, bukan one-hot encoded DataFrame
    target_labels_for_smote = full_data_encoded[target_columns_encoded].idxmax(axis=1).apply(lambda x: x.replace('NObeyesdad_', ''))


    # Simpan daftar kolom fitur yang sudah di-one-hot-encoded
    all_training_features_columns = features.columns.tolist()

    # Atasi ketidakseimbangan kelas data menggunakan SMOTE
    smote = SMOTE(random_state=42)
    features_resampled, target_resampled = smote.fit_resample(features, target_labels_for_smote)

    # Normalisasi atau Standarisasi Data
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features_resampled)

    return data, features_scaled, target_resampled, scaler, all_training_features_columns, numeric_cols

# Muat dan pra-proses data saat aplikasi dimulai
data_original_for_viz, features_scaled, target_resampled, scaler, all_training_features_columns, numeric_cols_for_viz = load_and_preprocess_data('ObesityDataSet.csv')

# --- PELATIHAN MODEL (Dijalankan sekali saat aplikasi dimulai) ---
@st.cache_resource # Menggunakan cache_resource untuk menyimpan model yang dilatih
def train_models(X_train, y_train):
    # Split data (jika perlu dilakukan di dalam fungsi yang di-cache_resource)
    # Namun, karena kita sudah punya features_scaled dan target_resampled dari load_and_preprocess_data,
    # kita bisa split di luar fungsi ini dan mengirimkan X_train, y_train langsung.
    # Untuk memastikan reproducibility training, split juga harus punya random_state

    # Inisialisasi model
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(random_state=42),
        'SVM': SVC(random_state=42)
    }

    trained_models = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        trained_models[name] = model

    # Hyperparameter Tuning untuk Random Forest
    param_grid = {
        'n_estimators': [50, 100],
        'max_depth': [None, 10, 20],
    }
    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    trained_models['Best Random Forest (Tuned)'] = best_model

    return trained_models

# Lakukan split data untuk pelatihan model
X_train, X_test, y_train, y_test = train_test_split(features_scaled, target_resampled, test_size=0.2, random_state=42)

# Latih model
trained_models = train_models(X_train, y_train)

# --- BAGIAN INPUT DATA UNTUK PREDIKSI ---
st.header("Input Data")
age = st.number_input("Usia (tahun)", min_value=0, max_value=120, value=25)
gender = st.selectbox("Jenis Kelamin", ["Male", "Female"])
height = st.number_input("Tinggi Badan (m)", min_value=0.5, max_value=2.5, value=1.70)
weight = st.number_input("Berat Badan (kg)", min_value=30, max_value=250, value=70)
family_history = st.selectbox("Apakah ada riwayat keluarga yang mengalami kelebihan berat badan?", ["Ya", "Tidak"])
FAVC = st.selectbox("Sering mengonsumsi makanan tinggi kalori?", ["Ya", "Tidak"])
FCVC = st.number_input("Frekuensi mengonsumsi sayuran (dalam sekali makan)", min_value=1, max_value=5, value=2)
NCP = st.number_input("Jumlah makan besar dalam sehari", min_value=1, max_value=10, value=3)
SMOKE = st.selectbox("Apakah Anda merokok?", ["Ya", "Tidak"])
CH2O = st.number_input("Jumlah air yang Anda minum setiap hari (liter)", min_value=0.5, max_value=5.0, value=2.0)
FAF = st.number_input("Frekuensi aktivitas fisik (dalam sekali seminggu)", min_value=0, max_value=7, value=1)
# Kolom CAEC dan MTRANS juga ada di dataset, perlu ditambahkan ke input
CAEC = st.selectbox("Konsumsi makanan antara waktu makan utama", ["Always", "Frequently", "Sometimes", "no"])
MTRANS = st.selectbox("Transportasi utama", ["Public_Transportation", "Automobile", "Walking", "Motorbike", "Bike"])
TUE = st.number_input("Penggunaan gawai (jam)", min_value=0.0, max_value=24.0, value=1.0) # Contoh TUE, asumsikan numerik

# Tombol untuk memprediksi
if st.button("Prediksi"):
    # Buat DataFrame input dari data yang dimasukkan pengguna
    input_data_df = pd.DataFrame([{
        'Age': age,
        'Gender': gender,
        'Height': height,
        'Weight': weight,
        'CALC': 'Sometimes', # CALC, SCC, family_history_with_overweight, TUE, CAEC, MTRANS
        'FAVC': FAVC,
        'FCVC': FCVC,
        'NCP': NCP,
        'SCC': 'no', # SCC belum ada di input, diisi default
        'SMOKE': SMOKE,
        'CH2O': CH2O,
        'family_history_with_overweight': family_history,
        'FAF': FAF,
        'TUE': TUE,
        'CAEC': CAEC,
        'MTRANS': MTRANS
    }])

    # Konversi kolom numerik di input_data_df yang mungkin masih object karena st.number_input
    # ini penting agar scaler bekerja
    for col in ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']:
        input_data_df[col] = pd.to_numeric(input_data_df[col], errors='coerce')


    # Lakukan one-hot encoding pada input_data_df
    input_data_encoded = pd.get_dummies(input_data_df, drop_first=True)

    # Sesuaikan kolom input_data_encoded dengan kolom fitur pelatihan
    # Tambahkan kolom yang hilang dengan nilai 0
    for col in all_training_features_columns:
        if col not in input_data_encoded.columns:
            input_data_encoded[col] = 0
    # Hapus kolom yang tidak ada di data pelatihan
    for col in input_data_encoded.columns:
        if col not in all_training_features_columns:
            input_data_encoded = input_data_encoded.drop(columns=[col])

    # Pastikan urutan kolom sesuai dengan data pelatihan
    input_data_aligned = input_data_encoded[all_training_features_columns]

    # Skalakan data input
    input_data_scaled = scaler.transform(input_data_aligned)

    # Lakukan prediksi menggunakan model terbaik
    prediction = trained_models['Best Random Forest (Tuned)'].predict(input_data_scaled)[0]

    st.success(f"Hasil Prediksi: {prediction}")

# --- BAGIAN VISUALISASI DATA ---
st.header("Visualisasi Data")
st.markdown("""
Berikut adalah visualisasi data yang dapat membantu Anda memahami distribusi tingkat obesitas.
""")

# Visualisasi distribusi target (menggunakan data_original_for_viz yang sudah bersih)
plt.figure(figsize=(10, 5))
sns.countplot(x='NObeyesdad', data=data_original_for_viz.copy())
plt.title('Distribusi Tingkat Obesitas')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
st.pyplot(plt)

# Visualisasi outlier dengan boxplot (menggunakan data_original_for_viz yang sudah bersih)
plt.figure(figsize=(12, 6))
sns.boxplot(data=data_original_for_viz[numeric_cols_for_viz], orient='h')
plt.title('Boxplot untuk Deteksi Outlier (Setelah Pra-pemrosesan)')
st.pyplot(plt)

# --- MENAMPILKAN DESKRIPSI DATA ---
st.header("Deskripsi Data")
# Ini sekarang akan menampilkan deskripsi data yang sudah dibersihkan
st.write(data_original_for_viz.describe())

# --- VISUALISASI PERBANDINGAN PERFORMA MODEL ---
st.header("Perbandingan Performa Model")

# Hitung akurasi model pada test set (menggunakan model yang sudah dilatih)
accuracies_before = {}
for name, model in trained_models.items():
    if 'Best' not in name:
        accuracies_before[name] = model.score(X_test, y_test)

plt.figure(figsize=(10, 5))
sns.barplot(x=list(accuracies_before.keys()), y=list(accuracies_before.values()))
plt.title('Akurasi Model Sebelum Penyetelan Hyperparameter')
plt.ylabel('Akurasi')
plt.ylim(0, 1)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
st.pyplot(plt)

accuracies_after = {
    'Best Random Forest (Tuned)': trained_models['Best Random Forest (Tuned)'].score(X_test, y_test)
}

plt.figure(figsize=(10, 5))
sns.barplot(x=list(accuracies_after.keys()), y=list(accuracies_after.values()))
plt.title('Akurasi Model Setelah Penyetelan Hyperparameter')
plt.ylabel('Akurasi')
plt.ylim(0, 1)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
st.pyplot(plt)

# Gabungkan akurasi sebelum dan sesudah tuning untuk plot gabungan
combined_accuracies = {**accuracies_before, **accuracies_after}

plt.figure(figsize=(12, 6))
sns.barplot(x=list(combined_accuracies.keys()), y=list(combined_accuracies.values()))
plt.title('Perbandingan Akurasi Model Sebelum dan Setelah Penyetelan Hyperparameter')
plt.ylabel('Akurasi')
plt.ylim(0, 1)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
st.pyplot(plt)

# --- KESIMPULAN ---
st.header("Kesimpulan")
st.markdown("""
Aplikasi ini memberikan estimasi tingkat obesitas berdasarkan input yang diberikan.
Silakan masukkan data Anda untuk melihat hasil prediksi.
""")